#################################################
#                                               #
# Written by Jean Urung and Olivier Duranteau   #
# Commented and annotated by Olivier Duranteau  #
# Corrected by Axel Abels and Benjamin Popoff   #
#                                               #
#################################################

###################################################################
# import of the different modules and display of the procedure # 
###################################################################


import pandas as pd         # a library for data manipulation and analysis, including tools for reading and writing data in various formats.
import numpy as np          # a library for numerical computing in Python.
import datetime             # a module for working with dates and times in Python.
from tqdm import tqdm       # a library for adding progress bars to Python loops and functions.
import pdb                  # a module for debugging Python code.
import os                   # a module for interacting with the operating system.
import sys                  # a module that provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.


import matplotlib.pyplot as plt                                 # a plotting library for Python.
from sklearn.impute import SimpleImputer                        # a class for imputing missing values in datasets.
from sklearn.experimental import enable_iterative_imputer       # a module that enables iterative imputation methods in scikit-learn.
from sklearn.impute import IterativeImputer,KNNImputer          # a class for iterative imputation of missing values & class for imputing missing values using k-nearest neighbors.

from sklearn.tree import DecisionTreeRegressor                  # a class for building decision tree regression models.
from sklearn.neural_network import MLPRegressor                 # a class for building multi-layer perceptron regression models.
from sklearn.pipeline import make_pipeline                      # a function for constructing pipelines of estimators.
from sklearn.preprocessing import StandardScaler                # a class for scaling data by removing the mean and scaling to unit variance.
from sklearn.ensemble import RandomForestRegressor              # a class for building random forest regression models.
from sklearn.ensemble import GradientBoostingRegressor          # a class for building gradient boosting regression models.
from sklearn.linear_model import LinearRegression               # a class for building linear regression models.
from sklearn import datasets, linear_model                      # a function for calculating the mean squared error between predicted and true values.

from sklearn.metrics import mean_squared_error, r2_score        # a function for calculating the R-squared score between predicted and true values.
import warnings
warnings.filterwarnings("ignore")


##############################################
# Retrait des valeurs nulles et des doublons #
##############################################

# Utilisation de la fonction pandas.DataFrame.dropna qui permet de retirer les valeurs manquantes des tableaux bruts
# Utilisation du module pandas
# Création de la fonction cleanDataframe qui supprime les valeurs manquantes puis dans un second temps qui supprime les doublons.

# The first function cleanDataframe takes a DataFrame as input and uses the pandas.DataFrame.dropna() method to remove any missing or null values, first from rows and then from columns. 
# The drop_duplicates() method is then used to remove any duplicate rows from the DataFrame, and the cleaned DataFrame is returned.
#


def cleanDataframe(df):
    df = df.dropna(axis=0, how="all")                                       # axis=0 permet de supprimer les lignes qui ont des valeurs vides ou nulles (ici les mesures biologiques)
    df = df.dropna(axis=1, how="all")                                       # axis=1 permet de supprimer les colonnes qui ont des valeurs vides ou nulles
    df = df.drop_duplicates()                                               # how = "all" permet de ne sélectionner que les colonnes ou les lignes qui ont toutes les valeurs comme vides ou nulles
    return df


###################################
# Nettoyage des dates et heures   #
###################################

# The second function strToTime is a helper function that takes a string date and time value and formats it to a datetime.datetime object using the datetime.datetime.strptime() method. 
# If the value cannot be formatted with the given format string, it prints an error message using the cprint() method and raises a ValueError.

def strToTime(value, format="%Y-%m-%d %H:%M:%S"):                           # création d'une fonction qui permet de convertir les dates au format str en format date + time
    if isinstance(value, str):                                              # utilisation de la fonction booléenne de Numpy
        if not isinstance(format, list):
            format = [format]
        for f in format:
            try:
                return datetime.datetime.strptime(value, f)                 # datetime.datetime = Une combinaison d'une date et d'une heure. Attributs : year, month, day, hour, minute, second, microsecond, et tzinfo.
            except:
                cprint(f, "FAILED with", value)                             # cprint permet d'afficher en couleur la présence d'un bug
        raise ValueError("no time format fit with", value)                  # définit l'exception ValueError en lui demandant de faire apparaitre "no time format fit with"
    else:
        return [strToTime(v, format) for v in value]


# nan = Not a number
# création de la fonction formatTime
# dans metavision il y avait 2 formats de date :
#                   soit avec des / et un format sous la forme d, m, y
#                   soit avec des - et un format sous la forme y, m, d


# The third function formatTime takes a string date and time value and formats it to a datetime.datetime object using the datetime.datetime() constructor. 
# It splits the date and time components from the input string, and then converts them to a list of integers that can be passed to the datetime.datetime() constructor. 
# If the input value is not a string, it simply returns it.

def formatTime(time):
    if str(time) == 'nan':
        return np.nan                                                       # permet d'éliminer toutes les valeurs n'étant pas des nombres dans la matrice
    if isinstance(time, list):
        return [formatTime(t) for t in time]
    else:
        try:
            date, heure = time.split(" ")
        except:
            date, heure = time, "00:00"
        d, m, y = date.split("/")                                           # split la date avec des/
        h, minute = heure.split(":")                                        # split les minutes avec des : 
        l = [int(y), int(m), int(d), int(h), int(minute)]
        time = datetime.datetime(*l)
        return time

#################################################################
# Récupération de la valeur précédente et suivante              #
# Utilisation du module pandas                                  #
# Utilisation de la formule pandas.index.duplicated             #
#       Défini avec le paramètre (keep=)                        #
# Utilisation de la formule pandas.index.get_loc.               #
#       Défini avec les paramètres (Key, method, tolerance)     #
#                                                               #
#################################################################

# The last function getDelaiValeurFromHour takes a DataFrame, a time value, and an index value as input, and returns a dictionary with the time delay and associated value for the closest time value before and after the input time value in the DataFrame. 
# It first formats the DataFrame's index using the formatTime function, and removes any duplicate time values using the drop_duplicates() method. 
# It then uses the pandas.index.get_loc() method to find the index values before and after the input time value, and calculates the time delay and associated value for each of them. 
# The dictionary with these values is then returned.


def getDelaiValeurFromHour(df, heure, val_index=3, return_index=False): # Axel: si les colonnes sont nomées il vaut mieux utiliser le nom de la colonne plutôt que l'indice (par souci de clarté)
    d = {}
    df.index = formatTime(list(df.index))
    df = df[~df.index.duplicated(keep='first')].sort_index()                # en cas de doublon, instruction de ne garder que la première valeur
    try:
        ind_before = df.index.get_loc(heure, method='ffill')                # ffil = find the PREVIOUS index value if no exact match
        d['delai avant'] = heure-df.index[ind_before]                       # permet de récupérer le délais en faisant la soustraction entre l'heure de la transfusion et l'heure du prélèvement
        d['valeur avant'] = df.iloc[ind_before, val_index]                  # permet de récupérer la valeur associée
        if return_index:
            d['index avant'] = ind_before
    except:
        pass
    try:
        ind_after = df.index.get_loc(heure, method='backfill')              # bfill = use NEXT index value if no exact match
        d['delai apres'] = df.index[ind_after]-heure                        # permet de récupérer le délais en faisant la soustraction entre l'heure du prélèvement @ l'heure de la transfusion
        d['valeur apres'] = df.iloc[ind_after, val_index]                   # permet de récupérer la valeur associée
        if return_index:
            d['index apres'] = ind_after
    except:
        pass
    return d

#########################################
# Définition du répertoire de travail   #
#########################################

datapath = "."

outdir = os.path.join(datapath, 'out')                                      # utilisation du sous-dossier 'out' pour enregistrer les fichiers
if not os.path.isfile(os.path.join(outdir, "output.csv")) or '-force' in sys.argv:      # forcer la création d'un fichier output.csv dans le sous dossier out

##########################################################################################################    
##########################################################################################################   
###########                                  Calcium                             #########################
########################################################################################################## 
##########################################################################################################
    
    #############################################################
    # Récupération des données dans les fichiers source brut    #
    # le Fichier transfusion.csv contient le nom 
    #############################################################
    csv_transfusion = pd.read_csv(os.path.join(datapath, "transfusion.csv"), encoding="latin-1", sep=';', index_col=[1, 2], header=5)
    csv_transfusion = cleanDataframe(csv_transfusion)
    patients_inTransfusion = np.unique(csv_transfusion.index.get_level_values(0).astype(str))

    csv_calcium = pd.read_csv(os.path.join(datapath, "calcium.csv"), encoding="latin-1", sep=';', index_col=[0, 3, 9], header=5)
    csv_calcium = cleanDataframe(csv_calcium)
    patients_inCalcium = np.unique(csv_calcium.index.get_level_values(0).astype(str)) 
    
    csv_transfu = pd.read_csv(os.path.join(datapath, "transfusion.csv"), encoding="latin-1", sep=';')

    ######################################################################

    transfusions = np.unique(csv_transfusion.index.get_level_values(1))             # permet de récupérer le numéro d'index 
    outdf = {}
    n = 0                                                                           # Commencer par le patient n=0 
    
    for patient in tqdm(patients_inTransfusion):                         
        n += 1                                                                      # aller au patient suivant avec un pas de 1
        get_calcium = patient in patients_inCalcium                                 # Récupère le nom du patient dans le fichier du calcium
        
        count = {t: 0 for t in transfusions}

        sub_trans = csv_transfusion.loc[patient]
        sub_calcium = None
        if get_calcium:
            sub_calcium = csv_calcium.loc[patient]
            
        for i, transfusion in enumerate(sub_trans.index):           # 'enumerate' permet de lancer une loop dans la requête
            count[transfusion] += 1                                 # Dit de faire un pas de 1 dans l'énumération
            transfusion += ' '+str(count[transfusion])              # 'transfusion' devient une chaine de charactère
            trans_heure = formatTime(sub_trans.iloc[i, 3])
            outdf[(patient, transfusion)] = {('heure', ''): trans_heure}
            # utilisation de la fonction 'zip' de python qui permet d'associer différentes tables entre elles
            for boolean, df in zip([get_calcium], [sub_calcium]):
                if boolean:
                    for param in np.unique(df.index.get_level_values(0)): # Axel: ici aussi utiliser le nom de la colonne dans get_level_values serait plus clair que d'utiliser 0
                        for k, v in getDelaiValeurFromHour(df.loc[param], trans_heure).items():
                            if param == ".TP %":                                                    # converti '.TP%' en 'TP%'
                                param = "TP %"                                                      # converti '.TP%' en 'TP%'
                            outdf[(patient, transfusion)][(param, k)] = v
                            
    outdf = pd.DataFrame.from_dict(outdf).T
    outdf = outdf.sort_index(ascending=False) 
    outdf.insert(0, "Centre", 'PSL', allow_duplicates=False)
    
    ca_data = outdf.reset_index()
    ca_data.columns = ['Nom', 'Type', 'centre', 'heure', 'ca_del_avt', 'ca_avt', 'ca_del_ap', 'ca_ap', 'caio_del_ap', 'caio_ap', 'caio_del_avt', 'caio_avt']
    
    ca_data['Type'] = ca_data['Type'].astype(str).str[:8]
    
    ca_data_df = pd.DataFrame(ca_data)

    ca_data_groupby = ca_data_df.groupby(by=['Nom', 'ca_avt', 'Type']).size()
    ca_data_groupby_df = ca_data_groupby.to_frame()
    ca_data_groupby_df_pivot = pd.pivot_table(ca_data_groupby_df, index=['Nom', 'ca_avt'], columns=['Type'], aggfunc=np.sum)
    ca_merged = ca_data_groupby_df_pivot.merge(ca_data, on=['Nom','ca_avt'])
    ca_drop = ca_merged.drop(['Type', 'caio_del_ap', 'caio_ap', 'caio_del_avt', 'caio_avt'], axis=1)
    ca_final = ca_drop.drop_duplicates(subset =['Nom', 'ca_avt'])
    ca_final = ca_final.rename(columns={ ca_final.columns[2]: "CGR", ca_final.columns[3]: "ATIII", ca_final.columns[4]: "Cell_Saver", 
                                        ca_final.columns[5]: "Clottafac", ca_final.columns[6]: "CPA", ca_final.columns[7]: "Novoseven" , 
                                        ca_final.columns[8]: "Fibrinogene", ca_final.columns[9]: "Melange_plq", ca_final.columns[10]: "PPSB", 
                                        ca_final.columns[11]: "PFC"})
    ca_final[['CGR', 'ATIII', 'Cell_Saver', 'Clottafac', 'CPA', 'Novoseven', 
              'Fibrinogene', 'Melange_plq', 'PPSB', 'PFC']] = ca_final[['CGR', 'ATIII', 'Cell_Saver', 'Clottafac', 'CPA', 'Novoseven', 
                                                                        'Fibrinogene', 'Melange_plq', 'PPSB', 'PFC']].fillna(0)
    ca_final['ca_avt'] = ca_final['ca_avt'].str.replace(',','.')
    ca_final['ca_ap'] = ca_final['ca_ap'].str.replace(',','.')
    
    df3 = ca_final.merge(csv_transfu, on='Nom', how='left')
    df3 = df3.drop_duplicates(subset =['Nom', 'ca_avt', 'ca_ap'])
    
    df4 = df3[df3["ca_del_avt"]<= "0 days 06:00:00"]
    df5 = df4[['Nom', 'ca_avt']]
    df5.columns = ['Nom', 'ca_avt_2']
    df6 = df3.merge(df5, on='Nom', how='left')
    df6 = df6.drop_duplicates(subset =['Nom', 'ca_avt', 'ca_ap'])
    
    df7 = df6[df6["ca_del_ap"]<= "0 days 06:00:00"]
    df8 = df7[['Nom', 'ca_ap']]
    df8.columns = ['Nom', 'ca_ap_2']
    df9 = df6.merge(df8, on='Nom', how='left')
    df9 = df9.drop_duplicates(subset =['Nom', 'ca_avt', 'ca_ap'])
    
    df10 = df9[df9["ca_ap_2"] != "0"]
    df11 = df10[['Nom', 'ca_ap_2']]
    df11.columns = ['Nom', 'ca_ap_3']
    df12 = df9.merge(df11, on='Nom', how='left')
    df12 = df12.drop_duplicates(subset =['Nom', 'ca_avt', 'ca_ap'])
    
    df13 = df12[df12["ca_avt_2"] != "0"]
    df14 = df13[['Nom', 'ca_avt_2']]
    df14.columns = ['Nom', 'ca_avt_3']
    df15 = df12.merge(df14, on='Nom', how='left')
    df15 = df15.drop_duplicates(subset =['Nom', 'ca_avt', 'ca_ap'])
    
    df = df15[['Nom', 'NIPP', 'CGR', 'ATIII', 'Cell_Saver', 'Clottafac', 'CPA',
       'Novoseven', 'Fibrinogene', 'Melange_plq', 'PPSB', 'PFC', 'centre', 'heure', 'Sexe', 'poids', 'taille', 'date_TH', 'heure incision', 
        'ca_del_avt', 'ca_del_ap', 'ca_ap_3', 'ca_avt_3']]
    
    df['heure incision']= pd.to_datetime(df['heure incision'])
    df['delais'] = df['heure'] - df['heure incision']
    
    dfJ1 = df[df['delais']<= "1 days"]
    dfS1 = df[df['delais']>= "1 days"]
    
    df.to_csv(os.path.join(outdir, "calcium_total.csv"), sep='\t', encoding="latin-1")
    dfJ1.to_csv(os.path.join(outdir, "calcium_J1.csv"), sep='\t', encoding="latin-1")
    dfS1.to_csv(os.path.join(outdir, "calcium_S1.csv"), sep='\t', encoding="latin-1")
    
################################################################################################################################
    
    
    imp = IterativeImputer(max_iter=10, random_state=0)
    dfJ1[['ca_avt_3', 'ca_ap_3']] = imp.fit_transform(dfJ1[['ca_avt_3', 'ca_ap_3']])
    
    tree = DecisionTreeRegressor()
    mlp = make_pipeline(
        StandardScaler(),
        MLPRegressor(hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),)
    rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
    XGboost = GradientBoostingRegressor(random_state=0)
    linregr = linear_model.LinearRegression()

    X = dfJ1[['CGR', 'ATIII', 'Cell_Saver', 'Clottafac', 'CPA', 'Novoseven', 'Fibrinogene', 'Melange_plq', 'PPSB', 'PFC', 'ca_avt_3']]
    y = dfJ1['ca_ap_3']
        
    tree.fit(X, y)
    mlp.fit(X, y)
    rf.fit(X,y)
    XGboost.fit(X,y)
    linregr.fit(X,y)

    pred_tree = tree.predict(X)
    pred_mlp = mlp.predict(X)
    pred_rf = rf.predict(X)
    pred_XGboost = XGboost.predict(X)
    pred_linregr = linregr.predict(X)
    
    plt.scatter(y, pred_tree, color="red", linewidth=0.1)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    print("Mean squared error: %.2f" % mean_squared_error(y, pred_tree))
    print("Coefficient of determination: %.2f" % r2_score(y, pred_tree))


    plt.scatter(y, pred_mlp, color="blue", linewidth=0.1)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    print("Mean squared error: %.2f" % mean_squared_error(y, pred_mlp))
    print("Coefficient of determination: %.2f" % r2_score(y, pred_mlp))

    plt.scatter(y, pred_rf, color="green", linewidth=0.1)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    print("Mean squared error: %.2f" % mean_squared_error(y, pred_rf))
    print("Coefficient of determination: %.2f" % r2_score(y, pred_rf))
    
    plt.scatter(y, pred_XGboost, color="yellow", linewidth=0.1)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    print("Mean squared error: %.2f" % mean_squared_error(y, pred_XGboost))
    print("Coefficient of determination: %.2f" % r2_score(y, pred_XGboost))

    plt.scatter(y, pred_linregr, color="black", linewidth=0.1)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    print("Mean squared error: %.2f" % mean_squared_error(y, pred_linregr))
    print("Coefficient of determination: %.2f" % r2_score(y, pred_linregr))
    
